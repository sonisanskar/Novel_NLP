{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('drugsComTrain_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9</td>\n",
       "      <td>20-May-12</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8</td>\n",
       "      <td>27-Apr-10</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5</td>\n",
       "      <td>14-Dec-09</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8</td>\n",
       "      <td>3-Nov-15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9</td>\n",
       "      <td>27-Nov-16</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161292</th>\n",
       "      <td>191035</td>\n",
       "      <td>Campral</td>\n",
       "      <td>Alcohol Dependence</td>\n",
       "      <td>\"I wrote my first report in Mid-October of 201...</td>\n",
       "      <td>10</td>\n",
       "      <td>31-May-15</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161293</th>\n",
       "      <td>127085</td>\n",
       "      <td>Metoclopramide</td>\n",
       "      <td>Nausea/Vomiting</td>\n",
       "      <td>\"I was given this in IV before surgey. I immed...</td>\n",
       "      <td>1</td>\n",
       "      <td>1-Nov-11</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161294</th>\n",
       "      <td>187382</td>\n",
       "      <td>Orencia</td>\n",
       "      <td>Rheumatoid Arthritis</td>\n",
       "      <td>\"Limited improvement after 4 months, developed...</td>\n",
       "      <td>2</td>\n",
       "      <td>15-Mar-14</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161295</th>\n",
       "      <td>47128</td>\n",
       "      <td>Thyroid desiccated</td>\n",
       "      <td>Underactive Thyroid</td>\n",
       "      <td>\"I&amp;#039;ve been on thyroid medication 49 years...</td>\n",
       "      <td>10</td>\n",
       "      <td>19-Sep-15</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161296</th>\n",
       "      <td>215220</td>\n",
       "      <td>Lubiprostone</td>\n",
       "      <td>Constipation, Chronic</td>\n",
       "      <td>\"I&amp;#039;ve had chronic constipation all my adu...</td>\n",
       "      <td>9</td>\n",
       "      <td>13-Dec-14</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161297 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueID                  drugName                     condition  \\\n",
       "0         206461                 Valsartan  Left Ventricular Dysfunction   \n",
       "1          95260                Guanfacine                          ADHD   \n",
       "2          92703                    Lybrel                 Birth Control   \n",
       "3         138000                Ortho Evra                 Birth Control   \n",
       "4          35696  Buprenorphine / naloxone             Opiate Dependence   \n",
       "...          ...                       ...                           ...   \n",
       "161292    191035                   Campral            Alcohol Dependence   \n",
       "161293    127085            Metoclopramide               Nausea/Vomiting   \n",
       "161294    187382                   Orencia          Rheumatoid Arthritis   \n",
       "161295     47128        Thyroid desiccated           Underactive Thyroid   \n",
       "161296    215220              Lubiprostone         Constipation, Chronic   \n",
       "\n",
       "                                                   review  rating       date  \\\n",
       "0       \"It has no side effect, I take it in combinati...       9  20-May-12   \n",
       "1       \"My son is halfway through his fourth week of ...       8  27-Apr-10   \n",
       "2       \"I used to take another oral contraceptive, wh...       5  14-Dec-09   \n",
       "3       \"This is my first time using any form of birth...       8   3-Nov-15   \n",
       "4       \"Suboxone has completely turned my life around...       9  27-Nov-16   \n",
       "...                                                   ...     ...        ...   \n",
       "161292  \"I wrote my first report in Mid-October of 201...      10  31-May-15   \n",
       "161293  \"I was given this in IV before surgey. I immed...       1   1-Nov-11   \n",
       "161294  \"Limited improvement after 4 months, developed...       2  15-Mar-14   \n",
       "161295  \"I&#039;ve been on thyroid medication 49 years...      10  19-Sep-15   \n",
       "161296  \"I&#039;ve had chronic constipation all my adu...       9  13-Dec-14   \n",
       "\n",
       "        usefulCount  \n",
       "0                27  \n",
       "1               192  \n",
       "2                17  \n",
       "3                10  \n",
       "4                37  \n",
       "...             ...  \n",
       "161292          125  \n",
       "161293           34  \n",
       "161294           35  \n",
       "161295           79  \n",
       "161296          116  \n",
       "\n",
       "[161297 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Abilify changed my life. There is hope. I was on Zoloft and Clonidine when I first started Abilify at the age of 15.. Zoloft for depression and Clondine to manage my complete rage. My moods were out of control. I was depressed and hopeless one second and then mean, irrational, and full of rage the next. My Dr. prescribed me 2mg of Abilify and from that point on I feel like I have been cured though I know I&#039;m not.. Bi-polar disorder is a constant battle. I know Abilify works for me because I have tried to get off it and lost complete control over my emotions. Went back on it and I was golden again.  I am on 5mg 2x daily. I am now 21 and better than I have ever been in the past. Only side effect is I like to eat a lot.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['review'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=pd.read_csv('drugsComTest_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train[:1000]   #taking only small part due to computational constraints\n",
    "data_test=data_test[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163740</td>\n",
       "      <td>Mirtazapine</td>\n",
       "      <td>Depression</td>\n",
       "      <td>\"I&amp;#039;ve tried a few antidepressants over th...</td>\n",
       "      <td>10</td>\n",
       "      <td>28-Feb-12</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206473</td>\n",
       "      <td>Mesalamine</td>\n",
       "      <td>Crohn's Disease, Maintenance</td>\n",
       "      <td>\"My son has Crohn&amp;#039;s disease and has done ...</td>\n",
       "      <td>8</td>\n",
       "      <td>17-May-09</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159672</td>\n",
       "      <td>Bactrim</td>\n",
       "      <td>Urinary Tract Infection</td>\n",
       "      <td>\"Quick reduction of symptoms\"</td>\n",
       "      <td>9</td>\n",
       "      <td>29-Sep-17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39293</td>\n",
       "      <td>Contrave</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"Contrave combines drugs that were used for al...</td>\n",
       "      <td>9</td>\n",
       "      <td>5-Mar-17</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97768</td>\n",
       "      <td>Cyclafem 1 / 35</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I have been on this birth control for one cyc...</td>\n",
       "      <td>9</td>\n",
       "      <td>22-Oct-15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>227344</td>\n",
       "      <td>Etonogestrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I got this for the same reason as everyone el...</td>\n",
       "      <td>1</td>\n",
       "      <td>23-Oct-13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>152480</td>\n",
       "      <td>Ciprofloxacin</td>\n",
       "      <td>Kidney Infections</td>\n",
       "      <td>\"I was given this medicine to prevent a kidney...</td>\n",
       "      <td>10</td>\n",
       "      <td>9-Nov-10</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>130786</td>\n",
       "      <td>Levonorgestrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I am a 42 year old mother of 1. I had Mirena ...</td>\n",
       "      <td>3</td>\n",
       "      <td>7-Nov-11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>38072</td>\n",
       "      <td>Adipex-P</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"These are all  fake commants for sure\"</td>\n",
       "      <td>1</td>\n",
       "      <td>23-Jan-16</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>81626</td>\n",
       "      <td>Liraglutide</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"Hi everyone! I started taking Saxenda 10 days...</td>\n",
       "      <td>10</td>\n",
       "      <td>7-Jul-17</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     uniqueID         drugName                     condition  \\\n",
       "0      163740      Mirtazapine                    Depression   \n",
       "1      206473       Mesalamine  Crohn's Disease, Maintenance   \n",
       "2      159672          Bactrim       Urinary Tract Infection   \n",
       "3       39293         Contrave                   Weight Loss   \n",
       "4       97768  Cyclafem 1 / 35                 Birth Control   \n",
       "..        ...              ...                           ...   \n",
       "295    227344     Etonogestrel                 Birth Control   \n",
       "296    152480    Ciprofloxacin             Kidney Infections   \n",
       "297    130786   Levonorgestrel                 Birth Control   \n",
       "298     38072         Adipex-P                   Weight Loss   \n",
       "299     81626      Liraglutide                   Weight Loss   \n",
       "\n",
       "                                                review  rating       date  \\\n",
       "0    \"I&#039;ve tried a few antidepressants over th...      10  28-Feb-12   \n",
       "1    \"My son has Crohn&#039;s disease and has done ...       8  17-May-09   \n",
       "2                        \"Quick reduction of symptoms\"       9  29-Sep-17   \n",
       "3    \"Contrave combines drugs that were used for al...       9   5-Mar-17   \n",
       "4    \"I have been on this birth control for one cyc...       9  22-Oct-15   \n",
       "..                                                 ...     ...        ...   \n",
       "295  \"I got this for the same reason as everyone el...       1  23-Oct-13   \n",
       "296  \"I was given this medicine to prevent a kidney...      10   9-Nov-10   \n",
       "297  \"I am a 42 year old mother of 1. I had Mirena ...       3   7-Nov-11   \n",
       "298            \"These are all  fake commants for sure\"       1  23-Jan-16   \n",
       "299  \"Hi everyone! I started taking Saxenda 10 days...      10   7-Jul-17   \n",
       "\n",
       "     usefulCount  \n",
       "0             22  \n",
       "1             17  \n",
       "2              3  \n",
       "3             35  \n",
       "4              4  \n",
       "..           ...  \n",
       "295            3  \n",
       "296           26  \n",
       "297            0  \n",
       "298           11  \n",
       "299           12  \n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    313\n",
       "9     160\n",
       "1     138\n",
       "8     120\n",
       "5      54\n",
       "7      53\n",
       "6      45\n",
       "3      43\n",
       "2      43\n",
       "4      31\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tags(x):   #converting the ratings column into 0's and 1's.  for binary classifier to take place\n",
    "    if(x<5):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['tag']=data_train['rating'].apply(lambda x: make_tags(x))\n",
    "data_test['tag']=data_test['rating'].apply(lambda x: make_tags(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9</td>\n",
       "      <td>20-May-12</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8</td>\n",
       "      <td>27-Apr-10</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5</td>\n",
       "      <td>14-Dec-09</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8</td>\n",
       "      <td>3-Nov-15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9</td>\n",
       "      <td>27-Nov-16</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>206872</td>\n",
       "      <td>Duac</td>\n",
       "      <td>Acne</td>\n",
       "      <td>\"The product is working so well for my athleti...</td>\n",
       "      <td>8</td>\n",
       "      <td>10-Oct-09</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>214615</td>\n",
       "      <td>Liothyronine</td>\n",
       "      <td>Underactive Thyroid</td>\n",
       "      <td>\"as a supplement to levothyroxine it has been ...</td>\n",
       "      <td>10</td>\n",
       "      <td>4-Jun-15</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>221357</td>\n",
       "      <td>Dextromethorphan</td>\n",
       "      <td>Cough</td>\n",
       "      <td>\"This worked great for my husband until he gav...</td>\n",
       "      <td>1</td>\n",
       "      <td>2-May-16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>135157</td>\n",
       "      <td>Dapsone</td>\n",
       "      <td>Acne</td>\n",
       "      <td>\"I have tried almost everything under the sun ...</td>\n",
       "      <td>10</td>\n",
       "      <td>2-Apr-10</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>224296</td>\n",
       "      <td>Levothyroxine</td>\n",
       "      <td>Underactive Thyroid</td>\n",
       "      <td>\"My thyroid test showed it to be under active,...</td>\n",
       "      <td>6</td>\n",
       "      <td>10-Apr-16</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     uniqueID                  drugName                     condition  \\\n",
       "0      206461                 Valsartan  Left Ventricular Dysfunction   \n",
       "1       95260                Guanfacine                          ADHD   \n",
       "2       92703                    Lybrel                 Birth Control   \n",
       "3      138000                Ortho Evra                 Birth Control   \n",
       "4       35696  Buprenorphine / naloxone             Opiate Dependence   \n",
       "..        ...                       ...                           ...   \n",
       "995    206872                      Duac                          Acne   \n",
       "996    214615              Liothyronine           Underactive Thyroid   \n",
       "997    221357          Dextromethorphan                         Cough   \n",
       "998    135157                   Dapsone                          Acne   \n",
       "999    224296             Levothyroxine           Underactive Thyroid   \n",
       "\n",
       "                                                review  rating       date  \\\n",
       "0    \"It has no side effect, I take it in combinati...       9  20-May-12   \n",
       "1    \"My son is halfway through his fourth week of ...       8  27-Apr-10   \n",
       "2    \"I used to take another oral contraceptive, wh...       5  14-Dec-09   \n",
       "3    \"This is my first time using any form of birth...       8   3-Nov-15   \n",
       "4    \"Suboxone has completely turned my life around...       9  27-Nov-16   \n",
       "..                                                 ...     ...        ...   \n",
       "995  \"The product is working so well for my athleti...       8  10-Oct-09   \n",
       "996  \"as a supplement to levothyroxine it has been ...      10   4-Jun-15   \n",
       "997  \"This worked great for my husband until he gav...       1   2-May-16   \n",
       "998  \"I have tried almost everything under the sun ...      10   2-Apr-10   \n",
       "999  \"My thyroid test showed it to be under active,...       6  10-Apr-16   \n",
       "\n",
       "     usefulCount  tag  \n",
       "0             27    1  \n",
       "1            192    1  \n",
       "2             17    1  \n",
       "3             10    1  \n",
       "4             37    1  \n",
       "..           ...  ...  \n",
       "995            9    1  \n",
       "996           66    1  \n",
       "997            6    0  \n",
       "998           39    1  \n",
       "999           10    1  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying sentence tokenizer\n",
    "import nltk.data \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') \n",
    "# Loading PunktSentenceTokenizer using English pickle file \n",
    "def make_sent_token(x):\n",
    "    return tokenizer.tokenize(x) \n",
    "#converting each paragraph into separate sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['sentence_token']=data_train['review'].apply(lambda x: make_sent_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sentence_token']=data_test['review'].apply(lambda x: make_sent_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "data_train.drop(columns=['uniqueID','date','usefulCount','condition','drugName'],inplace=True,axis=1)# dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(columns=['uniqueID','date','usefulCount','condition','drugName'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"It has no side effect, I take it in combinat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"My son is halfway through his fourth week of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"I used to take another oral contraceptive, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"This is my first time using any form of birt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"Suboxone has completely turned my life aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>\"The product is working so well for my athleti...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"The product is working so well for my athlet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\"as a supplement to levothyroxine it has been ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"as a supplement to levothyroxine it has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>\"This worked great for my husband until he gav...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"This worked great for my husband until he ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>\"I have tried almost everything under the sun ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"I have tried almost everything under the sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>\"My thyroid test showed it to be under active,...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"My thyroid test showed it to be under active...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  rating  tag  \\\n",
       "0    \"It has no side effect, I take it in combinati...       9    1   \n",
       "1    \"My son is halfway through his fourth week of ...       8    1   \n",
       "2    \"I used to take another oral contraceptive, wh...       5    1   \n",
       "3    \"This is my first time using any form of birth...       8    1   \n",
       "4    \"Suboxone has completely turned my life around...       9    1   \n",
       "..                                                 ...     ...  ...   \n",
       "995  \"The product is working so well for my athleti...       8    1   \n",
       "996  \"as a supplement to levothyroxine it has been ...      10    1   \n",
       "997  \"This worked great for my husband until he gav...       1    0   \n",
       "998  \"I have tried almost everything under the sun ...      10    1   \n",
       "999  \"My thyroid test showed it to be under active,...       6    1   \n",
       "\n",
       "                                        sentence_token  \n",
       "0    [\"It has no side effect, I take it in combinat...  \n",
       "1    [\"My son is halfway through his fourth week of...  \n",
       "2    [\"I used to take another oral contraceptive, w...  \n",
       "3    [\"This is my first time using any form of birt...  \n",
       "4    [\"Suboxone has completely turned my life aroun...  \n",
       "..                                                 ...  \n",
       "995  [\"The product is working so well for my athlet...  \n",
       "996  [\"as a supplement to levothyroxine it has been...  \n",
       "997  [\"This worked great for my husband until he ga...  \n",
       "998  [\"I have tried almost everything under the sun...  \n",
       "999  [\"My thyroid test showed it to be under active...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['no_of_sentences']=data_train['sentence_token'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['no_of_sentences']=data_test['sentence_token'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data_train['no_of_sentences'])##no of rows in sentence matrix which is to be feed in model(max number of sentence in any paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train[data_train['no_of_sentences']==92]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data_test['no_of_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length_of_sentence(x,y):\n",
    "    sen=x\n",
    "    nu=y\n",
    "    #print(sen)\n",
    "    ma=0\n",
    "    if(nu>1):\n",
    "        l=sen.split('.')\n",
    "        #print(l)\n",
    "        for i in range(len(l)):\n",
    "            k=l[i].replace(',','')\n",
    "            maxi=len(k.split())\n",
    "            #print(maxi)\n",
    "            if(maxi>ma):\n",
    "                ma=maxi\n",
    "        return ma\n",
    "    else:\n",
    "        return len(sen.split())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['max_words_in_sentence']=data_train.apply(lambda x: max_length_of_sentence(x.review,x.no_of_sentences),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['max_words_in_sentence']=data_test.apply(lambda x: max_length_of_sentence(x.review,x.no_of_sentences),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data_train['max_words_in_sentence'])## number of columns in the data to be feeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=max(data_train['no_of_sentences'])\n",
    "n=max(data_train['max_words_in_sentence'])\n",
    "\n",
    "#So each para will be converted to a m*n matrix   (where m is the number of sentence and n is number of words in each sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "data_train.rename(columns={'review':'title'},inplace=True)\n",
    "data_test.rename(columns={'review':'title'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major part starts here ..... Now converting the paragraph into required matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "def make_tokens(text):     ##Converting into single tokens in order to create the vocabulary\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "data_train['tokens']=data_train['title'].apply(lambda x: make_tokens(x))\n",
    "data_test['tokens']=data_test['title'].apply(lambda x: make_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [``, It, has, no, side, effect, ,, I, take, it...\n",
       "1      [``, My, son, is, halfway, through, his, fourt...\n",
       "2      [``, I, used, to, take, another, oral, contrac...\n",
       "3      [``, This, is, my, first, time, using, any, fo...\n",
       "4      [``, Suboxone, has, completely, turned, my, li...\n",
       "                             ...                        \n",
       "995    [``, The, product, is, working, so, well, for,...\n",
       "996    [``, as, a, supplement, to, levothyroxine, it,...\n",
       "997    [``, This, worked, great, for, my, husband, un...\n",
       "998    [``, I, have, tried, almost, everything, under...\n",
       "999    [``, My, thyroid, test, showed, it, to, be, un...\n",
       "Name: tokens, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103424 words total, with a vocabulary size of 7177\n",
      "Max sentence length is 501\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "para_max=max(training_sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7177"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAINING_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), char_level=False)\n",
    "tokenizer.fit_on_texts(data_train['title'])       # we assigned values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'and': 2,\n",
       " 'the': 3,\n",
       " 'to': 4,\n",
       " 'it': 5,\n",
       " 'my': 6,\n",
       " 'a': 7,\n",
       " '039': 8,\n",
       " 'for': 9,\n",
       " 'was': 10,\n",
       " 'have': 11,\n",
       " 'of': 12,\n",
       " 'this': 13,\n",
       " 'on': 14,\n",
       " 'in': 15,\n",
       " 'is': 16,\n",
       " 'me': 17,\n",
       " 'but': 18,\n",
       " 'had': 19,\n",
       " 'with': 20,\n",
       " 'that': 21,\n",
       " 'so': 22,\n",
       " 't': 23,\n",
       " 'been': 24,\n",
       " 'not': 25,\n",
       " 'after': 26,\n",
       " 'am': 27,\n",
       " 'no': 28,\n",
       " 'at': 29,\n",
       " 'as': 30,\n",
       " 'side': 31,\n",
       " 'about': 32,\n",
       " 'day': 33,\n",
       " 'm': 34,\n",
       " 'taking': 35,\n",
       " 'now': 36,\n",
       " 'all': 37,\n",
       " 'years': 38,\n",
       " 's': 39,\n",
       " 'pain': 40,\n",
       " 'started': 41,\n",
       " 'effects': 42,\n",
       " 'days': 43,\n",
       " 'first': 44,\n",
       " 've': 45,\n",
       " 'has': 46,\n",
       " 'get': 47,\n",
       " 'time': 48,\n",
       " 'be': 49,\n",
       " '2': 50,\n",
       " 'take': 51,\n",
       " 'out': 52,\n",
       " 'you': 53,\n",
       " 'only': 54,\n",
       " 'months': 55,\n",
       " 'just': 56,\n",
       " 'when': 57,\n",
       " 'very': 58,\n",
       " 'up': 59,\n",
       " 'or': 60,\n",
       " 'like': 61,\n",
       " 'can': 62,\n",
       " '3': 63,\n",
       " 'from': 64,\n",
       " 'would': 65,\n",
       " 'if': 66,\n",
       " 'more': 67,\n",
       " 'because': 68,\n",
       " 'one': 69,\n",
       " 'feel': 70,\n",
       " 'week': 71,\n",
       " 'back': 72,\n",
       " 'are': 73,\n",
       " 'took': 74,\n",
       " 'period': 75,\n",
       " '\\r': 76,\n",
       " 'weight': 77,\n",
       " 'will': 78,\n",
       " 'pill': 79,\n",
       " 'got': 80,\n",
       " 'weeks': 81,\n",
       " 'do': 82,\n",
       " 'medication': 83,\n",
       " 'which': 84,\n",
       " 'life': 85,\n",
       " 'doctor': 86,\n",
       " 'also': 87,\n",
       " 'then': 88,\n",
       " 'before': 89,\n",
       " 'bad': 90,\n",
       " 'did': 91,\n",
       " 'month': 92,\n",
       " 'much': 93,\n",
       " 'off': 94,\n",
       " '5': 95,\n",
       " 'really': 96,\n",
       " 'they': 97,\n",
       " 'still': 98,\n",
       " 'never': 99,\n",
       " 'some': 100,\n",
       " 'work': 101,\n",
       " 'anxiety': 102,\n",
       " 'since': 103,\n",
       " 'over': 104,\n",
       " 'well': 105,\n",
       " 'went': 106,\n",
       " 'control': 107,\n",
       " 'two': 108,\n",
       " 'year': 109,\n",
       " 'medicine': 110,\n",
       " 'better': 111,\n",
       " 'don': 112,\n",
       " 'go': 113,\n",
       " 'an': 114,\n",
       " 'than': 115,\n",
       " '6': 116,\n",
       " 'acne': 117,\n",
       " 'any': 118,\n",
       " 'every': 119,\n",
       " 'ago': 120,\n",
       " 'birth': 121,\n",
       " 'few': 122,\n",
       " 'tried': 123,\n",
       " 'didn': 124,\n",
       " '4': 125,\n",
       " 'even': 126,\n",
       " 'good': 127,\n",
       " 'other': 128,\n",
       " 'felt': 129,\n",
       " 'by': 130,\n",
       " 'sleep': 131,\n",
       " 'quot': 132,\n",
       " '10': 133,\n",
       " 'what': 134,\n",
       " 'great': 135,\n",
       " 'hours': 136,\n",
       " 'worked': 137,\n",
       " 'he': 138,\n",
       " 'again': 139,\n",
       " 'night': 140,\n",
       " '1': 141,\n",
       " 'help': 142,\n",
       " 'going': 143,\n",
       " 'severe': 144,\n",
       " 'works': 145,\n",
       " 'feeling': 146,\n",
       " 'far': 147,\n",
       " 'used': 148,\n",
       " 'last': 149,\n",
       " 'were': 150,\n",
       " 'depression': 151,\n",
       " 'almost': 152,\n",
       " 'skin': 153,\n",
       " 'dose': 154,\n",
       " 'getting': 155,\n",
       " 'sex': 156,\n",
       " 'there': 157,\n",
       " 'prescribed': 158,\n",
       " 'having': 159,\n",
       " 'mood': 160,\n",
       " 'made': 161,\n",
       " 'drug': 162,\n",
       " 'symptoms': 163,\n",
       " 'use': 164,\n",
       " 'times': 165,\n",
       " 'due': 166,\n",
       " 'mg': 167,\n",
       " 'cramps': 168,\n",
       " 'put': 169,\n",
       " 'being': 170,\n",
       " 'stomach': 171,\n",
       " 'while': 172,\n",
       " 'could': 173,\n",
       " 'amp': 174,\n",
       " 'lot': 175,\n",
       " 'nothing': 176,\n",
       " 'gain': 177,\n",
       " 'long': 178,\n",
       " 'old': 179,\n",
       " 'using': 180,\n",
       " 'away': 181,\n",
       " 'too': 182,\n",
       " '\\r\\r': 183,\n",
       " 'little': 184,\n",
       " 'think': 185,\n",
       " 'periods': 186,\n",
       " 'nausea': 187,\n",
       " 'try': 188,\n",
       " 'until': 189,\n",
       " 'ever': 190,\n",
       " 'always': 191,\n",
       " 'stopped': 192,\n",
       " 'experience': 193,\n",
       " 'see': 194,\n",
       " 'body': 195,\n",
       " 'effect': 196,\n",
       " 'down': 197,\n",
       " 'most': 198,\n",
       " 'around': 199,\n",
       " 'these': 200,\n",
       " 'we': 201,\n",
       " 'worse': 202,\n",
       " 'later': 203,\n",
       " 'your': 204,\n",
       " 'many': 205,\n",
       " 'lost': 206,\n",
       " '7': 207,\n",
       " 'horrible': 208,\n",
       " 'does': 209,\n",
       " 'pills': 210,\n",
       " 'stop': 211,\n",
       " 'normal': 212,\n",
       " 'say': 213,\n",
       " 'know': 214,\n",
       " 'headaches': 215,\n",
       " 'thing': 216,\n",
       " 'taken': 217,\n",
       " 'gone': 218,\n",
       " 'however': 219,\n",
       " 'working': 220,\n",
       " 'gave': 221,\n",
       " 'recommend': 222,\n",
       " 'happy': 223,\n",
       " 'eat': 224,\n",
       " 'noticed': 225,\n",
       " 'finally': 226,\n",
       " 'bleeding': 227,\n",
       " 'different': 228,\n",
       " 'without': 229,\n",
       " 'how': 230,\n",
       " 'next': 231,\n",
       " 'them': 232,\n",
       " 'during': 233,\n",
       " 'pounds': 234,\n",
       " 'less': 235,\n",
       " 'completely': 236,\n",
       " 'once': 237,\n",
       " 'best': 238,\n",
       " 'swings': 239,\n",
       " '8': 240,\n",
       " 'said': 241,\n",
       " 'daily': 242,\n",
       " 'found': 243,\n",
       " 'its': 244,\n",
       " 'blood': 245,\n",
       " 'she': 246,\n",
       " 'another': 247,\n",
       " 'worth': 248,\n",
       " 'problems': 249,\n",
       " 'make': 250,\n",
       " '15': 251,\n",
       " 'right': 252,\n",
       " 'everything': 253,\n",
       " 'morning': 254,\n",
       " 'need': 255,\n",
       " 'issues': 256,\n",
       " '30': 257,\n",
       " 'into': 258,\n",
       " 'love': 259,\n",
       " 'thought': 260,\n",
       " 'appetite': 261,\n",
       " 'want': 262,\n",
       " 'helped': 263,\n",
       " 'helps': 264,\n",
       " 'who': 265,\n",
       " 'through': 266,\n",
       " 'within': 267,\n",
       " 'able': 268,\n",
       " 'tired': 269,\n",
       " 'told': 270,\n",
       " 'today': 271,\n",
       " 'people': 272,\n",
       " 'drive': 273,\n",
       " 'switched': 274,\n",
       " 'loss': 275,\n",
       " 'give': 276,\n",
       " 'new': 277,\n",
       " '20': 278,\n",
       " 'shot': 279,\n",
       " 'anything': 280,\n",
       " 'panic': 281,\n",
       " 'three': 282,\n",
       " 'painful': 283,\n",
       " 'light': 284,\n",
       " 'second': 285,\n",
       " 'longer': 286,\n",
       " 'hope': 287,\n",
       " 'though': 288,\n",
       " 'trying': 289,\n",
       " 'myself': 290,\n",
       " 'diagnosed': 291,\n",
       " 'meds': 292,\n",
       " 'couple': 293,\n",
       " 'couldn': 294,\n",
       " 'gained': 295,\n",
       " 'face': 296,\n",
       " 'things': 297,\n",
       " 'bit': 298,\n",
       " 'full': 299,\n",
       " 'experienced': 300,\n",
       " 'everyone': 301,\n",
       " 'same': 302,\n",
       " 'where': 303,\n",
       " 'wasn': 304,\n",
       " 'should': 305,\n",
       " 'headache': 306,\n",
       " 'seems': 307,\n",
       " 'sure': 308,\n",
       " 'water': 309,\n",
       " 'came': 310,\n",
       " 'dry': 311,\n",
       " '12': 312,\n",
       " 'start': 313,\n",
       " 'll': 314,\n",
       " 'worst': 315,\n",
       " 'sometimes': 316,\n",
       " 'something': 317,\n",
       " 'product': 318,\n",
       " 'extremely': 319,\n",
       " 'pregnant': 320,\n",
       " 'changed': 321,\n",
       " 'way': 322,\n",
       " 'low': 323,\n",
       " 'diarrhea': 324,\n",
       " 'decided': 325,\n",
       " 'starting': 326,\n",
       " 'depressed': 327,\n",
       " 'infection': 328,\n",
       " 'definitely': 329,\n",
       " 'half': 330,\n",
       " 'fine': 331,\n",
       " 'bed': 332,\n",
       " 'patch': 333,\n",
       " 'plan': 334,\n",
       " 'dr': 335,\n",
       " 'along': 336,\n",
       " 'haven': 337,\n",
       " 'terrible': 338,\n",
       " 'caused': 339,\n",
       " 'actually': 340,\n",
       " 'b': 341,\n",
       " 'd': 342,\n",
       " 'sick': 343,\n",
       " 'high': 344,\n",
       " 'twice': 345,\n",
       " 'attacks': 346,\n",
       " 'already': 347,\n",
       " 'migraines': 348,\n",
       " 'eating': 349,\n",
       " 'energy': 350,\n",
       " 'mouth': 351,\n",
       " 'increased': 352,\n",
       " 'given': 353,\n",
       " 'began': 354,\n",
       " 'problem': 355,\n",
       " 'effective': 356,\n",
       " 'makes': 357,\n",
       " 'although': 358,\n",
       " 'keep': 359,\n",
       " 'enough': 360,\n",
       " 'heavy': 361,\n",
       " 'reviews': 362,\n",
       " 'doesn': 363,\n",
       " 'why': 364,\n",
       " 'pretty': 365,\n",
       " 'relief': 366,\n",
       " 'treatment': 367,\n",
       " 'cramping': 368,\n",
       " 'spotting': 369,\n",
       " 'lower': 370,\n",
       " 'may': 371,\n",
       " 'cold': 372,\n",
       " 'pressure': 373,\n",
       " 'point': 374,\n",
       " 'major': 375,\n",
       " 'lbs': 376,\n",
       " 'insomnia': 377,\n",
       " 'suffered': 378,\n",
       " '9': 379,\n",
       " 'come': 380,\n",
       " 'reason': 381,\n",
       " 'past': 382,\n",
       " 'anymore': 383,\n",
       " 'hair': 384,\n",
       " 'several': 385,\n",
       " 'absolutely': 386,\n",
       " 'read': 387,\n",
       " 'those': 388,\n",
       " '24': 389,\n",
       " 'here': 390,\n",
       " 'person': 391,\n",
       " 'each': 392,\n",
       " 'soon': 393,\n",
       " 'awful': 394,\n",
       " 'chronic': 395,\n",
       " 'insurance': 396,\n",
       " 'extreme': 397,\n",
       " 'home': 398,\n",
       " 'hard': 399,\n",
       " 'cymbalta': 400,\n",
       " 'heart': 401,\n",
       " 'drink': 402,\n",
       " 'needed': 403,\n",
       " 'mild': 404,\n",
       " 'everyday': 405,\n",
       " 'change': 406,\n",
       " 'clear': 407,\n",
       " 'rsquo': 408,\n",
       " 'usually': 409,\n",
       " 'maybe': 410,\n",
       " 'him': 411,\n",
       " 'medications': 412,\n",
       " 'amazing': 413,\n",
       " 'disorder': 414,\n",
       " 'dosage': 415,\n",
       " 'food': 416,\n",
       " 'med': 417,\n",
       " 'under': 418,\n",
       " 'thoughts': 419,\n",
       " 'anyone': 420,\n",
       " 'surgery': 421,\n",
       " 'least': 422,\n",
       " 'doctors': 423,\n",
       " 'early': 424,\n",
       " 'insertion': 425,\n",
       " '10mg': 426,\n",
       " 'overall': 427,\n",
       " 'pack': 428,\n",
       " 'notice': 429,\n",
       " 'cleared': 430,\n",
       " 'diet': 431,\n",
       " 'itching': 432,\n",
       " 'deal': 433,\n",
       " 'his': 434,\n",
       " 'called': 435,\n",
       " 'immediately': 436,\n",
       " 'slight': 437,\n",
       " 'constipation': 438,\n",
       " 'pregnancy': 439,\n",
       " 'takes': 440,\n",
       " 'negative': 441,\n",
       " 'believe': 442,\n",
       " 'anti': 443,\n",
       " 'wish': 444,\n",
       " '25': 445,\n",
       " 'prescription': 446,\n",
       " 'fast': 447,\n",
       " 'minutes': 448,\n",
       " 'hour': 449,\n",
       " 'else': 450,\n",
       " 'results': 451,\n",
       " 'let': 452,\n",
       " 'free': 453,\n",
       " 'ended': 454,\n",
       " 'third': 455,\n",
       " 'zoloft': 456,\n",
       " 'recently': 457,\n",
       " 'done': 458,\n",
       " 'experiencing': 459,\n",
       " 'cause': 460,\n",
       " 'wanted': 461,\n",
       " 'left': 462,\n",
       " 'both': 463,\n",
       " 'hungry': 464,\n",
       " 'reading': 465,\n",
       " 'plus': 466,\n",
       " 'difference': 467,\n",
       " 'lasted': 468,\n",
       " 'bc': 469,\n",
       " 'honestly': 470,\n",
       " 'remember': 471,\n",
       " '100': 472,\n",
       " 'yet': 473,\n",
       " 'c': 474,\n",
       " 'tell': 475,\n",
       " 'stay': 476,\n",
       " 'migraine': 477,\n",
       " 'inserted': 478,\n",
       " 'end': 479,\n",
       " 're': 480,\n",
       " 'sleeping': 481,\n",
       " 'wouldn': 482,\n",
       " 'removed': 483,\n",
       " 'exercise': 484,\n",
       " 'cream': 485,\n",
       " 'regular': 486,\n",
       " 'others': 487,\n",
       " 'sensitive': 488,\n",
       " 'highly': 489,\n",
       " 'mirena': 490,\n",
       " 'implant': 491,\n",
       " 'throat': 492,\n",
       " 'son': 493,\n",
       " 'became': 494,\n",
       " 'school': 495,\n",
       " '3rd': 496,\n",
       " 'constant': 497,\n",
       " 'etc': 498,\n",
       " 'lose': 499,\n",
       " 'upset': 500,\n",
       " 'level': 501,\n",
       " 'break': 502,\n",
       " 'injection': 503,\n",
       " 'burning': 504,\n",
       " 'hrs': 505,\n",
       " 'recommended': 506,\n",
       " 'miracle': 507,\n",
       " 'four': 508,\n",
       " 'hoping': 509,\n",
       " 'her': 510,\n",
       " 'lexapro': 511,\n",
       " 'yrs': 512,\n",
       " 'yes': 513,\n",
       " 'per': 514,\n",
       " 'big': 515,\n",
       " 'whole': 516,\n",
       " 'super': 517,\n",
       " 'sore': 518,\n",
       " 'klonopin': 519,\n",
       " '2nd': 520,\n",
       " 'age': 521,\n",
       " 'improvement': 522,\n",
       " 'fatigue': 523,\n",
       " 'find': 524,\n",
       " 'normally': 525,\n",
       " 'entire': 526,\n",
       " 'prozac': 527,\n",
       " 'part': 528,\n",
       " 'pains': 529,\n",
       " 'scared': 530,\n",
       " 'family': 531,\n",
       " 'small': 532,\n",
       " '40': 533,\n",
       " 'stopping': 534,\n",
       " 'saved': 535,\n",
       " 'lots': 536,\n",
       " 'continue': 537,\n",
       " '17': 538,\n",
       " 'eyes': 539,\n",
       " 'positive': 540,\n",
       " 'glad': 541,\n",
       " 'none': 542,\n",
       " 'cost': 543,\n",
       " 'except': 544,\n",
       " 'nexplanon': 545,\n",
       " 'walk': 546,\n",
       " 'someone': 547,\n",
       " '11': 548,\n",
       " 'er': 549,\n",
       " 'chest': 550,\n",
       " 'okay': 551,\n",
       " 'hospital': 552,\n",
       " 'anxious': 553,\n",
       " 'stress': 554,\n",
       " 'guess': 555,\n",
       " 'nauseous': 556,\n",
       " 'crying': 557,\n",
       " 'husband': 558,\n",
       " '18': 559,\n",
       " 'between': 560,\n",
       " 'might': 561,\n",
       " 'attack': 562,\n",
       " 'quickly': 563,\n",
       " 'expensive': 564,\n",
       " 'goes': 565,\n",
       " 'literally': 566,\n",
       " 'taste': 567,\n",
       " 'emotional': 568,\n",
       " 'decreased': 569,\n",
       " 'job': 570,\n",
       " '5mg': 571,\n",
       " 'wake': 572,\n",
       " '50': 573,\n",
       " 'function': 574,\n",
       " 'minor': 575,\n",
       " 'head': 576,\n",
       " '14': 577,\n",
       " 'easy': 578,\n",
       " 'gets': 579,\n",
       " 'vomiting': 580,\n",
       " 'zero': 581,\n",
       " 'making': 582,\n",
       " 'thinking': 583,\n",
       " 'yeast': 584,\n",
       " '0': 585,\n",
       " 'antibiotics': 586,\n",
       " 'medicines': 587,\n",
       " 'constantly': 588,\n",
       " 'self': 589,\n",
       " 'woke': 590,\n",
       " 'crazy': 591,\n",
       " 'dizziness': 592,\n",
       " 'doing': 593,\n",
       " 'generic': 594,\n",
       " 'such': 595,\n",
       " 'talk': 596,\n",
       " 'thank': 597,\n",
       " '16': 598,\n",
       " 'effexor': 599,\n",
       " 'withdrawal': 600,\n",
       " 'gotten': 601,\n",
       " 'happened': 602,\n",
       " 'drugs': 603,\n",
       " 'asleep': 604,\n",
       " 'late': 605,\n",
       " 'suffering': 606,\n",
       " 'kind': 607,\n",
       " 'wait': 608,\n",
       " 'non': 609,\n",
       " 'isn': 610,\n",
       " 'hurt': 611,\n",
       " 'aches': 612,\n",
       " 'currently': 613,\n",
       " '50mg': 614,\n",
       " 'looked': 615,\n",
       " 'boyfriend': 616,\n",
       " 'course': 617,\n",
       " 'focus': 618,\n",
       " 'wonder': 619,\n",
       " 'worry': 620,\n",
       " 'system': 621,\n",
       " 'wonderful': 622,\n",
       " 'depo': 623,\n",
       " 'stuff': 624,\n",
       " 'hot': 625,\n",
       " 'bathroom': 626,\n",
       " 'prior': 627,\n",
       " 'often': 628,\n",
       " 'till': 629,\n",
       " 'six': 630,\n",
       " 'female': 631,\n",
       " 'straight': 632,\n",
       " 'bloating': 633,\n",
       " '20mg': 634,\n",
       " 'add': 635,\n",
       " 'red': 636,\n",
       " 'swelling': 637,\n",
       " 'huge': 638,\n",
       " 'suffer': 639,\n",
       " 'baby': 640,\n",
       " 'rather': 641,\n",
       " 'switch': 642,\n",
       " 'brand': 643,\n",
       " 'breast': 644,\n",
       " 'hardly': 645,\n",
       " 'cycle': 646,\n",
       " 'similar': 647,\n",
       " 'form': 648,\n",
       " 'money': 649,\n",
       " 'bowel': 650,\n",
       " 'abilify': 651,\n",
       " 'moody': 652,\n",
       " 'probably': 653,\n",
       " 'amount': 654,\n",
       " 'barely': 655,\n",
       " 'tablets': 656,\n",
       " 'evening': 657,\n",
       " 'especially': 658,\n",
       " 'luck': 659,\n",
       " 'kept': 660,\n",
       " 'god': 661,\n",
       " 'figure': 662,\n",
       " 'neck': 663,\n",
       " 'pm': 664,\n",
       " 'brain': 665,\n",
       " 'mind': 666,\n",
       " 'drinking': 667,\n",
       " 'cut': 668,\n",
       " 'including': 669,\n",
       " 'wellbutrin': 670,\n",
       " 'ok': 671,\n",
       " 'breakouts': 672,\n",
       " 'issue': 673,\n",
       " 'fever': 674,\n",
       " 'won': 675,\n",
       " 'friend': 676,\n",
       " 'quite': 677,\n",
       " 'eventually': 678,\n",
       " 'seen': 679,\n",
       " 'cannot': 680,\n",
       " 'improved': 681,\n",
       " 'skyla': 682,\n",
       " 'run': 683,\n",
       " 'slightly': 684,\n",
       " 'sugar': 685,\n",
       " 'levels': 686,\n",
       " 'either': 687,\n",
       " 'throughout': 688,\n",
       " 'slept': 689,\n",
       " 'nearly': 690,\n",
       " 'intense': 691,\n",
       " 'compared': 692,\n",
       " 'supply': 693,\n",
       " 'health': 694,\n",
       " 'hopefully': 695,\n",
       " 'serious': 696,\n",
       " 'losing': 697,\n",
       " 'uti': 698,\n",
       " 'itchy': 699,\n",
       " 'drowsy': 700,\n",
       " 'state': 701,\n",
       " 'feels': 702,\n",
       " '90': 703,\n",
       " 'hands': 704,\n",
       " 'unprotected': 705,\n",
       " 'bipolar': 706,\n",
       " 'reaction': 707,\n",
       " 'seem': 708,\n",
       " 'type': 709,\n",
       " 'smoking': 710,\n",
       " 'doc': 711,\n",
       " 'xr': 712,\n",
       " 'weird': 713,\n",
       " 'causes': 714,\n",
       " 'thyroid': 715,\n",
       " 'iud': 716,\n",
       " 'sit': 717,\n",
       " 'fear': 718,\n",
       " 'eye': 719,\n",
       " 'continued': 720,\n",
       " 'look': 721,\n",
       " 'instead': 722,\n",
       " 'worried': 723,\n",
       " 'viibryd': 724,\n",
       " 'tenderness': 725,\n",
       " 'wear': 726,\n",
       " 'fe': 727,\n",
       " '21': 728,\n",
       " 'libido': 729,\n",
       " 'leg': 730,\n",
       " 'initial': 731,\n",
       " 'test': 732,\n",
       " 'hate': 733,\n",
       " 'moderate': 734,\n",
       " 'affects': 735,\n",
       " 'cough': 736,\n",
       " 'trouble': 737,\n",
       " 'keeps': 738,\n",
       " 'broke': 739,\n",
       " 'finished': 740,\n",
       " 'arm': 741,\n",
       " 'asked': 742,\n",
       " 'forget': 743,\n",
       " 'coming': 744,\n",
       " 'followed': 745,\n",
       " 'looking': 746,\n",
       " 'please': 747,\n",
       " 'become': 748,\n",
       " 'own': 749,\n",
       " 'quit': 750,\n",
       " '22': 751,\n",
       " 'breasts': 752,\n",
       " 'inside': 753,\n",
       " 'developed': 754,\n",
       " 'seroquel': 755,\n",
       " 'joint': 756,\n",
       " 'loved': 757,\n",
       " 'loestrin': 758,\n",
       " 'seeing': 759,\n",
       " '40mg': 760,\n",
       " 'must': 761,\n",
       " 'kick': 762,\n",
       " 'adhd': 763,\n",
       " 'beginning': 764,\n",
       " 'pay': 765,\n",
       " 'allergic': 766,\n",
       " 'xanax': 767,\n",
       " 'waiting': 768,\n",
       " 'sweat': 769,\n",
       " 'antibiotic': 770,\n",
       " 'slowly': 771,\n",
       " 'combination': 772,\n",
       " 'downside': 773,\n",
       " 'turned': 774,\n",
       " 'ready': 775,\n",
       " 'minimal': 776,\n",
       " 'extra': 777,\n",
       " 'affect': 778,\n",
       " '25mg': 779,\n",
       " 'unfortunately': 780,\n",
       " 'feet': 781,\n",
       " 'fibromyalgia': 782,\n",
       " 'totally': 783,\n",
       " 'friends': 784,\n",
       " 'causing': 785,\n",
       " 'process': 786,\n",
       " 'office': 787,\n",
       " 'infections': 788,\n",
       " 'easily': 789,\n",
       " 'yesterday': 790,\n",
       " 'mother': 791,\n",
       " 'finding': 792,\n",
       " 'care': 793,\n",
       " 'ask': 794,\n",
       " 'afraid': 795,\n",
       " 'sweats': 796,\n",
       " 'nuvaring': 797,\n",
       " 'liver': 798,\n",
       " 'clean': 799,\n",
       " 'calm': 800,\n",
       " 'cry': 801,\n",
       " 'condition': 802,\n",
       " 'rid': 803,\n",
       " 'arthritis': 804,\n",
       " 'expected': 805,\n",
       " 'real': 806,\n",
       " 'wonders': 807,\n",
       " '00': 808,\n",
       " 'dreams': 809,\n",
       " 'following': 810,\n",
       " 'irregular': 811,\n",
       " 'forehead': 812,\n",
       " 'short': 813,\n",
       " 'lighter': 814,\n",
       " 'muscle': 815,\n",
       " 'saw': 816,\n",
       " 'previous': 817,\n",
       " 'sinus': 818,\n",
       " 'rest': 819,\n",
       " 'pristiq': 820,\n",
       " 'yaz': 821,\n",
       " 'name': 822,\n",
       " 'foods': 823,\n",
       " 'methadone': 824,\n",
       " 'suggested': 825,\n",
       " 'oral': 826,\n",
       " 'spent': 827,\n",
       " 'truly': 828,\n",
       " 'complete': 829,\n",
       " 'attention': 830,\n",
       " 'mostly': 831,\n",
       " 'giving': 832,\n",
       " '2016': 833,\n",
       " 'spot': 834,\n",
       " 'house': 835,\n",
       " 'medical': 836,\n",
       " 'comes': 837,\n",
       " 'male': 838,\n",
       " 'loose': 839,\n",
       " 'psychiatrist': 840,\n",
       " 'added': 841,\n",
       " 'happier': 842,\n",
       " 'tremors': 843,\n",
       " 'considering': 844,\n",
       " 'sweating': 845,\n",
       " 'easier': 846,\n",
       " 'usual': 847,\n",
       " 'nose': 848,\n",
       " 'five': 849,\n",
       " 'patches': 850,\n",
       " 'lack': 851,\n",
       " 'sad': 852,\n",
       " 'pharmacy': 853,\n",
       " 'knew': 854,\n",
       " 'awesome': 855,\n",
       " 'hasn': 856,\n",
       " '2015': 857,\n",
       " 'lithium': 858,\n",
       " 'satisfied': 859,\n",
       " 'teeth': 860,\n",
       " 'redness': 861,\n",
       " 'seriously': 862,\n",
       " 'seemed': 863,\n",
       " 'stayed': 864,\n",
       " 'increase': 865,\n",
       " 'dermatologist': 866,\n",
       " 'says': 867,\n",
       " 'relieve': 868,\n",
       " 'dealing': 869,\n",
       " 'miserable': 870,\n",
       " 'bronchitis': 871,\n",
       " 'helping': 872,\n",
       " 'uncomfortable': 873,\n",
       " 'fall': 874,\n",
       " 'a1c': 875,\n",
       " 'their': 876,\n",
       " 'single': 877,\n",
       " 'active': 878,\n",
       " 'suicidal': 879,\n",
       " 'hell': 880,\n",
       " 'true': 881,\n",
       " 'rate': 882,\n",
       " '60': 883,\n",
       " 'physical': 884,\n",
       " 'ocd': 885,\n",
       " 'unable': 886,\n",
       " 'lo': 887,\n",
       " 'suboxone': 888,\n",
       " 'mean': 889,\n",
       " 'legs': 890,\n",
       " '28': 891,\n",
       " 'mental': 892,\n",
       " 'close': 893,\n",
       " 'returned': 894,\n",
       " 'send': 895,\n",
       " 'manic': 896,\n",
       " 'live': 897,\n",
       " 'discomfort': 898,\n",
       " 'im': 899,\n",
       " 'yourself': 900,\n",
       " 'spinal': 901,\n",
       " 'injections': 902,\n",
       " 'ibs': 903,\n",
       " 'doses': 904,\n",
       " 'healthy': 905,\n",
       " 'stand': 906,\n",
       " 'wrong': 907,\n",
       " 'vl': 908,\n",
       " 'post': 909,\n",
       " 'hit': 910,\n",
       " 'feelings': 911,\n",
       " 'women': 912,\n",
       " 'otherwise': 913,\n",
       " 'spots': 914,\n",
       " 'ovulation': 915,\n",
       " 'ms': 916,\n",
       " 'cialis': 917,\n",
       " 'upon': 918,\n",
       " 'effectiveness': 919,\n",
       " 'move': 920,\n",
       " 'together': 921,\n",
       " 'fact': 922,\n",
       " 'saver': 923,\n",
       " 'celexa': 924,\n",
       " 'kids': 925,\n",
       " 'significantly': 926,\n",
       " 'bladder': 927,\n",
       " 'previously': 928,\n",
       " 'procedure': 929,\n",
       " 'case': 930,\n",
       " 'children': 931,\n",
       " '45': 932,\n",
       " 'abdominal': 933,\n",
       " 'significant': 934,\n",
       " 'bloated': 935,\n",
       " 'dont': 936,\n",
       " 'bactrim': 937,\n",
       " 'happens': 938,\n",
       " 'switching': 939,\n",
       " 'vyvanse': 940,\n",
       " 'putting': 941,\n",
       " 'strange': 942,\n",
       " 'march': 943,\n",
       " 'fell': 944,\n",
       " 'constipated': 945,\n",
       " 'size': 946,\n",
       " 'concerta': 947,\n",
       " 'meal': 948,\n",
       " 'bumps': 949,\n",
       " 'ritalin': 950,\n",
       " 'bull': 951,\n",
       " 'available': 952,\n",
       " 'brown': 953,\n",
       " 'discharge': 954,\n",
       " 'subsided': 955,\n",
       " 'chance': 956,\n",
       " 'method': 957,\n",
       " '100mg': 958,\n",
       " 'vision': 959,\n",
       " 'depressants': 960,\n",
       " 'inability': 961,\n",
       " 'comfortable': 962,\n",
       " 'matter': 963,\n",
       " 'number': 964,\n",
       " 'despite': 965,\n",
       " 'saturday': 966,\n",
       " 'review': 967,\n",
       " 'chin': 968,\n",
       " 'nasty': 969,\n",
       " 'throwing': 970,\n",
       " 'breathing': 971,\n",
       " 'angry': 972,\n",
       " 'unbearable': 973,\n",
       " '80': 974,\n",
       " 'morphine': 975,\n",
       " 'round': 976,\n",
       " 'received': 977,\n",
       " 'against': 978,\n",
       " 'benefits': 979,\n",
       " 'dizzy': 980,\n",
       " 'tiredness': 981,\n",
       " 'trulicity': 982,\n",
       " 'metformin': 983,\n",
       " 'gas': 984,\n",
       " 'disease': 985,\n",
       " 'lifesaver': 986,\n",
       " 'ten': 987,\n",
       " 'cancer': 988,\n",
       " 'patient': 989,\n",
       " 'react': 990,\n",
       " 'place': 991,\n",
       " 'wondering': 992,\n",
       " 'miss': 993,\n",
       " 'breaking': 994,\n",
       " 'colonoscopy': 995,\n",
       " 'fully': 996,\n",
       " 'finish': 997,\n",
       " 'thankful': 998,\n",
       " 'mine': 999,\n",
       " 'gaining': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def make_train_seq(x):\n",
    "    return tokenizer.texts_to_sequences(x)\n",
    "data_train['train_seq']=data_train['sentence_token'].apply(lambda x:make_train_seq(x) )\n",
    "data_test['train_seq']=data_test['sentence_token'].apply(lambda x:make_train_seq(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[5, 46, 28, 31, 196, 1, 51, 5, 15, 772, 12, 3...\n",
       "1      [[6, 493, 16, 1849, 266, 434, 1332, 71, 12, 18...\n",
       "2      [[1, 148, 4, 51, 247, 826, 1064, 84, 19, 728, ...\n",
       "3      [[13, 16, 6, 44, 48, 180, 118, 648, 12, 121, 1...\n",
       "4      [[888, 46, 236, 774, 6, 85, 199], [1, 70, 1854...\n",
       "                             ...                        \n",
       "995    [[3, 318, 16, 220, 22, 105, 9, 6, 2088, 2224, ...\n",
       "996    [[30, 7, 1611, 4, 2189, 5, 46, 24, 7, 140, 2, ...\n",
       "997    [[13, 137, 135, 9, 6, 558, 189, 138, 221, 17, ...\n",
       "998    [[1, 11, 123, 152, 253, 418, 3, 1352, 30, 1259...\n",
       "999    [[6, 715, 732, 1390, 5, 4, 49, 418, 878, 335, ...\n",
       "Name: train_seq, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_train['train_seq'])   # here every para has been encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def padding(x):    #now padding each sentence to a length of n...number of columns\n",
    "    MAX_SENTENCE_LENGTH=n  #(no of columns)\n",
    "    return pad_sequences(x,maxlen=MAX_SENTENCE_LENGTH,padding='post')\n",
    "\n",
    "data_train['padded']=data_train['train_seq'].apply(lambda x:padding(x))\n",
    "data_test['padded']=data_test['train_seq'].apply(lambda x:padding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   45,   19,  176,   18,  249,   20,    3, 3200,  497, 1067,\n",
       "          15,    6, 1344,  174,  890,  174, 2327,  174, 2328,  146,   15,\n",
       "           6, 1344,  174,  890,  144,  284, 2329,   28,  261,  174,  498,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_train.padded[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5848, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
    " EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    " train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_para(x):     #92 cross 192 matrix of a paragraph.   (m*n)\n",
    "    l=len(x)\n",
    "    h=m-l    #no. of extra rows to be added\n",
    "    z=[0]*h*n       #1D vector(#addding extra lines for zeroes as padding)\n",
    "    z=np.reshape(z,(h,n))    #reshaping it to match the dimension of paragraph\n",
    "    s=x.tolist()+z.tolist()\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['full_para']=data_train['padded'].apply(lambda x : make_full_para(x))\n",
    "data_test['full_para']=data_test['padded'].apply(lambda x : make_full_para(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[5, 46, 28, 31, 196, 1, 51, 5, 15, 772, 12, 3...\n",
       "1      [[6, 493, 16, 1849, 266, 434, 1332, 71, 12, 18...\n",
       "2      [[1, 148, 4, 51, 247, 826, 1064, 84, 19, 728, ...\n",
       "3      [[13, 16, 6, 44, 48, 180, 118, 648, 12, 121, 1...\n",
       "4      [[888, 46, 236, 774, 6, 85, 199, 0, 0, 0, 0, 0...\n",
       "                             ...                        \n",
       "995    [[3, 318, 16, 220, 22, 105, 9, 6, 2088, 2224, ...\n",
       "996    [[30, 7, 1611, 4, 2189, 5, 46, 24, 7, 140, 2, ...\n",
       "997    [[13, 137, 135, 9, 6, 558, 189, 138, 221, 17, ...\n",
       "998    [[1, 11, 123, 152, 253, 418, 3, 1352, 30, 1259...\n",
       "999    [[6, 715, 732, 1390, 5, 4, 49, 418, 878, 335, ...\n",
       "Name: full_para, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.full_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1d_para(x):\n",
    "    l=[]\n",
    "    for i in x:\n",
    "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
    "    return l\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['single_d_array']=data_train['full_para'].apply(lambda x: create_1d_para(x) )\n",
    "data_test['single_d_array']=data_test['full_para'].apply(lambda x: create_1d_para(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data=np.array(data_train['single_d_array'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=np.array(data_test['single_d_array'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    8,   45, ...,    0,    0,    0],\n",
       "       [   6,  493,   46, ...,    0,    0,    0],\n",
       "       [1242, 4878,   12, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   27,    7, ...,    0,    0,    0],\n",
       "       [ 200,   73,   37, ...,    0,    0,    0],\n",
       "       [2370,  301,    0, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_cnn_data=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=data_train['tag'].values\n",
    "#y_test=data_test['tag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM,SpatialDropout1D,Reshape\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv2D, GlobalMaxPooling2D,MaxPool2D,MaxPool3D,GlobalAveragePooling2D,Conv3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,4]\n",
    "num_filters = 32\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=m*n\n",
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((m, n, 300))(x)\n",
    "    #print(x)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 3),\n",
    "                                                                                    activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 1), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2), activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    maxpool_0 = MaxPool2D()(conv_0)\n",
    "    maxpool_0=Flatten()(maxpool_0)\n",
    "    maxpool_1 = MaxPool2D()(conv_1)\n",
    "    maxpool_1=Flatten()(maxpool_1)\n",
    "    #maxpool_2 = MaxPool2D()(conv_2)\n",
    "    #maxpool_3 = MaxPool2D()(conv_3)\n",
    "    \n",
    "    maxpool_4 = MaxPool2D()(conv_4)\n",
    "    maxpool_4=Flatten()(maxpool_4)\n",
    "    maxpool_5 = MaxPool2D()(conv_5)\n",
    "    maxpool_5=Flatten()(maxpool_5)\n",
    "    #maxpool_6 = MaxPool2D()(conv_6)\n",
    "    #maxpool_6=Flatten()(maxpool_6)\n",
    "    #maxpool_7 = MaxPool2D()(conv_7)\n",
    "   # maxpool_7=Flatten()(maxpool_7)\n",
    "        \n",
    "    w=concatenate([maxpool_4, maxpool_5],axis=1)\n",
    "    w=Flatten()(w)\n",
    "    z = concatenate([maxpool_0, maxpool_1],axis=1)\n",
    "    \n",
    "    \n",
    "    z = Flatten()(z)\n",
    "    z=concatenate([w,z],axis=1)\n",
    "    z=Dense(units=128,activation=\"relu\")(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model09=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3925)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 3925, 300)    1754400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 3925, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 25, 157, 300) 0           spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 24, 157, 32)  19232       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 24, 156, 32)  38432       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 25, 156, 32)  19232       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 25, 155, 32)  28832       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 12, 78, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 12, 78, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 12, 78, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 12, 77, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 29952)        0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 29952)        0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 29952)        0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 29568)        0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 59904)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 59520)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 59904)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 59520)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 119424)       0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          15286400    concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,146,657\n",
      "Trainable params: 17,146,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model09.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "900/900 [==============================] - 65s 72ms/sample - loss: 0.6450 - accuracy: 0.6878 - val_loss: 0.6100 - val_accuracy: 0.7300\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 64s 71ms/sample - loss: 0.5453 - accuracy: 0.7467 - val_loss: 0.6091 - val_accuracy: 0.7300\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 61s 67ms/sample - loss: 0.5040 - accuracy: 0.7467 - val_loss: 0.5792 - val_accuracy: 0.7300\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 62s 68ms/sample - loss: 0.4288 - accuracy: 0.7556 - val_loss: 0.5916 - val_accuracy: 0.7400\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 60s 67ms/sample - loss: 0.3192 - accuracy: 0.8367 - val_loss: 0.6092 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 60s 67ms/sample - loss: 0.1752 - accuracy: 0.9500 - val_loss: 0.7244 - val_accuracy: 0.7600\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 61s 68ms/sample - loss: 0.0962 - accuracy: 0.9678 - val_loss: 0.7897 - val_accuracy: 0.7200\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "hist = model09.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,validation_split=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  38,  28, ...,   0,   0,   0],\n",
       "       [  6, 645,  19, ...,   0,   0,   0],\n",
       "       [  1, 153,   4, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [236, 236, 189, ...,   0,   0,   0],\n",
       "       [  1,  45,  36, ...,   0,   0,   0],\n",
       "       [741,  24,  66, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model09.predict(test_cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred=model09.predict(test_cnn_data.tolist())\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i][0]<0.5):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "def check_metric(output_class_pred,original_ans):\n",
    "    rightly_predicted=0\n",
    "    TP=0\n",
    "    for i in range(len(y_test)):\n",
    "        if(original_ans[i]==output_class_pred[i]):\n",
    "            rightly_predicted+=1\n",
    "        \n",
    "        \n",
    "    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n",
    "    print('TP',TP)\n",
    "    accuracy=rightly_predicted/len(y_test)\n",
    "    print(classification_report(original_ans,output_class_pred))\n",
    "    print(confusion_matrix(original_ans,output_class_pred))\n",
    "    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n",
    "    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n",
    "    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n",
    "    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recalll=TP/(FN+TP)\n",
    "    F1=2*precision*recalll/(precision+recalll)\n",
    "    sensiti=TP/(TP+FN)\n",
    "    specifici=TN/(TN+FP)\n",
    "    numerator=TP*TN - FP*FN\n",
    "    \n",
    "    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "    MCc=numerator/denominator\n",
    "    G_mean1=np.sqrt(sensiti*precision)\n",
    "    G_mean2=np.sqrt(sensiti*specifici)\n",
    "    print('precision:' ,TP/(TP+FP))\n",
    "    print('recall:',TP/(FN+TP))\n",
    "    print(\"F1:\",F1)\n",
    "    print(\"Specificity:\",TN/(TN+FP))\n",
    "    print(\"Sensitivity \",TP/(TP+FN))\n",
    "    print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "    print(\"MCC :\",MCc)\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    tp=[]\n",
    "    fp=[]\n",
    "    fn=[]\n",
    "    tn=[]\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    recall.append(recalll)\n",
    "    f1.append(F1)\n",
    "    specificity.append(specifici)\n",
    "    sensitivity.append(sensiti)\n",
    "    GMean1.append(G_mean1)\n",
    "    Gmean2.append(G_mean2)\n",
    "    MCC.append(MCc)\n",
    "    tp.append(TP)\n",
    "    fp.append(FP)\n",
    "    tn.append(TN)\n",
    "    fn.append(FN)\n",
    "    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall_acuracy: 0.7066666666666667\n",
      "TP 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.27      0.35        89\n",
      "           1       0.74      0.89      0.81       211\n",
      "\n",
      "    accuracy                           0.71       300\n",
      "   macro avg       0.63      0.58      0.58       300\n",
      "weighted avg       0.67      0.71      0.67       300\n",
      "\n",
      "[[ 24  65]\n",
      " [ 23 188]]\n",
      "precision: 0.7430830039525692\n",
      "recall: 0.8909952606635071\n",
      "F1: 0.8103448275862069\n",
      "Specificity: 0.2696629213483146\n",
      "Sensitivity  0.8909952606635071\n",
      "G-mean1: 0.8136850955998526\n",
      "G-mean2 0.49017179121000465\n",
      "MCC : 0.20189701109442576\n"
     ]
    }
   ],
   "source": [
    "resi=check_metric(output_class_pred,original_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "resi.to_csv('results.csv', mode='w', index = False, header=resi.columns,columns=resi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now perparing training data for yoon kim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_line_para(x):\n",
    "    l=[]\n",
    "    for i in x:\n",
    "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
    "    return l\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskarsoni/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['create_single_line_para']=data_train['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
    "data_test['create_single_line_para']=data_test['train_seq'].apply(lambda x: create_single_line_para(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [5, 46, 28, 31, 196, 1, 51, 5, 15, 772, 12, 31...\n",
       "1      [6, 493, 16, 1849, 266, 434, 1332, 71, 12, 185...\n",
       "2      [1, 148, 4, 51, 247, 826, 1064, 84, 19, 728, 7...\n",
       "3      [13, 16, 6, 44, 48, 180, 118, 648, 12, 121, 10...\n",
       "4      [888, 46, 236, 774, 6, 85, 199, 1, 70, 1854, 1...\n",
       "                             ...                        \n",
       "995    [3, 318, 16, 220, 22, 105, 9, 6, 2088, 2224, 1...\n",
       "996    [30, 7, 1611, 4, 2189, 5, 46, 24, 7, 140, 2, 3...\n",
       "997    [13, 137, 135, 9, 6, 558, 189, 138, 221, 17, 4...\n",
       "998    [1, 11, 123, 152, 253, 418, 3, 1352, 30, 1259,...\n",
       "999    [6, 715, 732, 1390, 5, 4, 49, 418, 878, 335, 3...\n",
       "Name: create_single_line_para, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_train['create_single_line_para'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoon_kim_train_data=np.array(data_train['create_single_line_para'].tolist())\n",
    "yoon_kim_train_data=pad_sequences(yoon_kim_train_data,maxlen=para_max,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoon_kim_test_data=np.array(data_test['create_single_line_para'].tolist())\n",
    "yoon_kim_test_data=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    8,   45, ...,    0,    0,    0],\n",
       "       [   6,  493,   46, ...,    0,    0,    0],\n",
       "       [1242, 4878,   12, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   27,    7, ...,    0,    0,    0],\n",
       "       [ 200,   73,   37, ...,    0,    0,    0],\n",
       "       [2370,  301,    1, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_y=train_y[[0,1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=para_max \n",
    "max_sequence_length=para_max\n",
    "MAX_SEQUENCE_LENGTH=para_max\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "\n",
    "#model3 yoon kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    #pool = MaxPooling1D(pool_size=2)(conv)\n",
    "\n",
    "    #if extra_conv==True:\n",
    "        #x = Dropout(0.01)(l_merge)  \n",
    "    #else:\n",
    "        # Original Yoon Kim model\n",
    "        #x = Dropout(0.001)(pool)\n",
    "    x = Flatten()(l_merge)\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 501)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 501, 300)     1754400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 499, 100)     90100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 498, 100)     120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 497, 100)     150100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 249, 100)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 249, 100)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 248, 100)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 746, 100)     0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 74600)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 74600)        0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            149202      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,263,902\n",
      "Trainable params: 2,263,902\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                 True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=yoon_kim_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data=yoon_kim_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "900/900 [==============================] - 23s 26ms/sample - loss: 0.6682 - acc: 0.7178 - val_loss: 0.6011 - val_acc: 0.7300\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 0.4442 - acc: 0.7711 - val_loss: 0.5664 - val_acc: 0.7700\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 23s 26ms/sample - loss: 0.3173 - acc: 0.8633 - val_loss: 0.6353 - val_acc: 0.7400\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 25s 27ms/sample - loss: 0.1853 - acc: 0.9422 - val_loss: 0.7942 - val_acc: 0.7400\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 20s 22ms/sample - loss: 0.1250 - acc: 0.9689 - val_loss: 0.8126 - val_acc: 0.7300\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 22s 24ms/sample - loss: 0.0709 - acc: 0.9856 - val_loss: 0.6227 - val_acc: 0.7700\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "hist = model1.fit(training_data, trains_y,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model1.predict(testing_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "#output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    m=max(y_test[i])\n",
    "    if(y_test[i].index(m)==0):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "def check_metric(output_class_pred,original_ans):\n",
    "    rightly_predicted=0\n",
    "    TP=0\n",
    "    for i in range(len(y_test)):\n",
    "        if(original_ans[i]==output_class_pred[i]):\n",
    "            rightly_predicted+=1\n",
    "        \n",
    "        \n",
    "    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n",
    "    print('TP',TP)\n",
    "    accuracy=rightly_predicted/len(y_test)\n",
    "    print(classification_report(original_ans,output_class_pred))\n",
    "    print(confusion_matrix(original_ans,output_class_pred))\n",
    "    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n",
    "    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n",
    "    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n",
    "    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recalll=TP/(FN+TP)\n",
    "    F1=2*precision*recalll/(precision+recalll)\n",
    "    sensiti=TP/(TP+FN)\n",
    "    specifici=TN/(TN+FP)\n",
    "    numerator=TP*TN - FP*FN\n",
    "    \n",
    "    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "    MCc=numerator/denominator\n",
    "    G_mean1=np.sqrt(sensiti*precision)\n",
    "    G_mean2=np.sqrt(sensiti*specifici)\n",
    "    print('precision:' ,TP/(TP+FP))\n",
    "    print('recall:',TP/(FN+TP))\n",
    "    print(\"F1:\",F1)\n",
    "    print(\"Specificity:\",TN/(TN+FP))\n",
    "    print(\"Sensitivity \",TP/(TP+FN))\n",
    "    print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "    print(\"MCC :\",MCc)\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    tp=[]\n",
    "    fp=[]\n",
    "    fn=[]\n",
    "    tn=[]\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    recall.append(recalll)\n",
    "    f1.append(F1)\n",
    "    specificity.append(specifici)\n",
    "    sensitivity.append(sensiti)\n",
    "    GMean1.append(G_mean1)\n",
    "    Gmean2.append(G_mean2)\n",
    "    MCC.append(MCc)\n",
    "    tp.append(TP)\n",
    "    fp.append(FP)\n",
    "    tn.append(TN)\n",
    "    fn.append(FN)\n",
    "    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall_acuracy: 0.6833333333333333\n",
      "TP 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.13      0.20        89\n",
      "           1       0.71      0.91      0.80       211\n",
      "\n",
      "    accuracy                           0.68       300\n",
      "   macro avg       0.56      0.52      0.50       300\n",
      "weighted avg       0.62      0.68      0.62       300\n",
      "\n",
      "[[ 12  77]\n",
      " [ 18 193]]\n",
      "precision: 0.7148148148148148\n",
      "recall: 0.9146919431279621\n",
      "F1: 0.8024948024948024\n",
      "Specificity: 0.1348314606741573\n",
      "Sensitivity  0.9146919431279621\n",
      "G-mean1: 0.8086008607091741\n",
      "G-mean2 0.3511826458679676\n",
      "MCC : 0.07540570559672281\n"
     ]
    }
   ],
   "source": [
    "resi=check_metric(output_class_pred,original_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "resi.to_csv('results.csv', mode='w', index = False, header=resi.columns,columns=resi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
